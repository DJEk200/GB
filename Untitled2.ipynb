{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOL4ilkHfnHLUwLJQGrwg5q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJEk200/GB/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxwdgaPYGO-4"
      },
      "source": [
        "**Ответы к уроку № 3**\r\n",
        "\r\n",
        "**1**. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?\r\n",
        "\r\n",
        "Варианты усреднения метрик качества особенно полезны для решения задач многоклассовой классификации. Самый простой и стандардтный вариант - это macro. В нем берутся усредненные значения для каждого из классов. Macro не делает различий между более и менее важными классами. Также, macro фокусируется на распозновании класса, а не усредненных значениях precision/recall для всей модели. \r\n",
        "Micro вариант усредняет precision/recall для всей многоклассовой модели, но не уделяет внимания распределению классов в данных. Weighted вариант присваивает веса для каждого из классов, исходя из их распределения в данных.  \r\n",
        "\r\n",
        "**2**. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?\r\n",
        "\r\n",
        "-- XGBoost - это eXtreme Gradient Boosting, т.е. в основе данной модели лежит градиентный бустинг. XGBoost посчитывает похожеть между элементами и производит стрижку результирующих деревьев, основываясь на гиперпараметр гамма. \r\n",
        "Прирост информации в XGBoost подсчитывается, как разница между суммой веток и вершины.\r\n",
        "\r\n",
        "-- LightGBM - это облегченная версия градиентного бустинга, которая заостряет внимание на ошибках и не использует всю выборку. Также, эта модель группирует разреженные признаки, типичные для бинарного формата машинного обучения, а также признаки с близкими диапазонами значений.\r\n",
        "\r\n",
        "-- Catboost - это быстрая модель градиентного бустинга, построенная на симметричных разветвлениях. Выдерживая симметрию, модель решает проблемы классификации быстрее аналогов. \r\n",
        "В дополнение, эта модель сама переводит признаки в нужный формат без использования функции get_dummies и имеет встроенные алгоритмы, препятствующие переобучению.  \r\n"
      ]
    }
  ]
}